{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import prework as pwk\n",
    "import string\n",
    "import sys, os, io\n",
    "\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.cross_validation import train_test_split, cross_val_score, KFold\n",
    "from sklearn.feature_extraction.text import TfidfTransformer, CountVectorizer\n",
    "from sklearn.linear_model import LogisticRegression, SGDClassifier\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.grid_search import GridSearchCV\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import recall_score, accuracy_score, precision_score, roc_auc_score\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "from gensim.models.doc2vec import LabeledSentence, Doc2Vec, Word2Vec\n",
    "from gensim.utils import simple_preprocess, simple_tokenize\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-0.0646220009883\n"
     ]
    }
   ],
   "source": [
    "sentences = [['first', 'sentence'], ['second', 'sentence']]\n",
    "model1 = Word2Vec(sentences, min_count=1)\n",
    "#print model1.similarity('third','sentence')\n",
    "\n",
    "#let this be the model from which you want to reset\n",
    "sentences = [['third', 'sentence'], ['fourth', 'sentence']]\n",
    "model2 = Word2Vec(sentences, min_count=1)\n",
    "model1.reset_from(model2)\n",
    "print model1.similarity('third','sentence')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "stdout = sys.stdout\n",
    "\n",
    "reload(sys)\n",
    "sys.setdefaultencoding('utf-8')\n",
    "\n",
    "sys.stdout = stdout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "root = 'data/pilot'\n",
    "#root = 'data/ICNALE/Unmerged_classified'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Raw Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "209\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>doc_id</th>\n",
       "      <th>path</th>\n",
       "      <th>author_code</th>\n",
       "      <th>essay_content</th>\n",
       "      <th>label</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>data/pilot/W_CHN_PTJ0_021_A2_0.txt</td>\n",
       "      <td>W_CHN_PTJ0_021_A2_0.txt</td>\n",
       "      <td>﻿I agree that it is important for college stud...</td>\n",
       "      <td>CHN</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   doc_id                                path              author_code  \\\n",
       "0       1  data/pilot/W_CHN_PTJ0_021_A2_0.txt  W_CHN_PTJ0_021_A2_0.txt   \n",
       "\n",
       "                                       essay_content label  target  \n",
       "0  ﻿I agree that it is important for college stud...   CHN       1  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_orig = pwk.load_data(root, 1, 'CHN', 'ENS')\n",
    "df_orig.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(209, 6)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_orig.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_1 = df_orig.drop('path', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pd.set_option('display.max_colwidth', -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.52153110047846885"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(df_1['label']=='CHN')*1./df_1.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#df_1[['essay_content','label']]\n",
    "from gensim.models import Doc2Vec\n",
    "from gensim.models.doc2vec import TaggedDocument\n",
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_data, test_data = train_test_split(df_1, test_size=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_tagged = train_data.apply(\\\n",
    "    #lambda r: TaggedDocument(words=tokenize_text(r['essay_content']), tags=[r.label]), axis=1)\n",
    "    lambda r: TaggedDocument(words=simple_preprocess(r['essay_content']), tags=[r.label]), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_tagged = test_data.apply(\n",
    "    lambda r: TaggedDocument(words=simple_preprocess(r['essay_content']), tags=[r.label]), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TaggedDocument(words=[u'many', u'parents', u'want', u'their', u'students', u'to', u'be', u'more', u'responsible', u'many', u'students', u'want', u'to', u'have', u'more', u'freedom', u'one', u'way', u'for', u'both', u'groups', u'to', u'have', u'winning', u'solution', u'is', u'for', u'the', u'college', u'students', u'to', u'have', u'part', u'time', u'job', u'so', u'think', u'it', u'is', u'good', u'thing', u'for', u'college', u'students', u'to', u'have', u'part', u'time', u'job', u'the', u'students', u'for', u'their', u'part', u'demonstrate', u'to', u'their', u'parents', u'that', u'they', u'are', u'responsible', u'and', u'ready', u'enough', u'to', u'have', u'part', u'time', u'job', u'and', u'to', u'function', u'effectively', u'in', u'the', u'business', u'world', u'the', u'parents', u'will', u'see', u'that', u'their', u'children', u'are', u'becoming', u'responsible', u'and', u'be', u'willing', u'to', u'give', u'them', u'more', u'leeway', u'to', u'do', u'as', u'they', u'please', u'they', u'do', u'not', u'have', u'to', u'be', u'so', u'controlling', u'of', u'the', u'children', u'because', u'the', u'children', u'are', u'showing', u'them', u'that', u'they', u'have', u'what', u'it', u'takes', u'to', u'make', u'decisions', u'for', u'themselves', u'next', u'many', u'students', u'need', u'to', u'make', u'money', u'college', u'is', u'very', u'expensive', u'and', u'so', u'many', u'of', u'them', u'have', u'to', u'find', u'part', u'time', u'job', u'so', u'that', u'they', u'can', u'pay', u'to', u'go', u'to', u'school', u'it', u'is', u'not', u'so', u'hard', u'to', u'find', u'part', u'time', u'job', u'in', u'college', u'because', u'in', u'every', u'college', u'town', u'there', u'are', u'many', u'businesses', u'that', u'are', u'very', u'used', u'to', u'the', u'college', u'town', u'environment', u'in', u'which', u'many', u'college', u'students', u'work', u'they', u'are', u'usually', u'very', u'understanding', u'of', u'the', u'need', u'for', u'flexibility', u'in', u'college', u'employee', u'schedule'], tags=['ENS'])"
      ]
     },
     "execution_count": 157,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_tagged.values[9]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainsent = train_tagged.values\n",
    "testsent = test_tagged.values\n",
    "\n",
    "#simple gensim doc2vec api\n",
    "doc2vec_model = Doc2Vec(trainsent, size=5, iter=20, dm=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "#simple_preprocess(train_tagged.values[6].words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train_tagged.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.39795774, -0.22801702, -2.42773151,  1.22191691, -0.74887758], dtype=float32)"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc2vec_model.infer_vector(train_tagged.values[5].words, steps=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.31919047, -0.96560031, -2.26834393,  1.14743721, -0.8488428 ], dtype=float32)"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc2vec_model.infer_vector(train_tagged.values[7].words, steps=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#train_targets, train_regressors = zip(\n",
    "    #*[(doc.tags[0], doc2vec_model.infer_vector(doc.words, steps=20)) for doc in trainsent])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cv_data, test_data = train_test_split(df_1, test_size=0.1)\n",
    "cv_train_data, cv_test_data = train_test_split(cv_data, test_size=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def tokenize_text(text):\n",
    "    tokens = []\n",
    "    for sent in nltk.sent_tokenize(text):\n",
    "        for word in nltk.word_tokenize(sent):\n",
    "            if len(word) < 2:\n",
    "                continue\n",
    "            tokens.append(word.lower())\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def tag_docs(docs, col):\n",
    "    tagged = docs.apply(\\\n",
    "            lambda r: TaggedDocument(words=tokenize_text(r[col]), tags=[r.label]), axis=1)\n",
    "    return tagged"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train_doc2vec_model_new(tagged_docs):    \n",
    "    sents = tagged_docs.values\n",
    "    # train doc2vec model to get vector representation of documents\n",
    "    doc2vec_model = Doc2Vec(sents, size=5, iter=20, dm=1)\n",
    "    return doc2vec_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def vec_for_learning(doc2vec_model, tagged_docs):\n",
    "    sents = tagged_docs.values\n",
    "    targets, regressors = zip(\\\n",
    "            *[(doc.tags[0], doc2vec_model.infer_vector(doc.words, steps=20)) for doc in sents])\n",
    "    return targets, regressors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X = cv_data.reindex(range(cv_data.shape[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "0.945945945946\n"
     ]
    }
   ],
   "source": [
    "#from sklearn.cro import KFold\n",
    "kf = KFold(n=cv_data.shape[0], n_folds=5)\n",
    "X = cv_data\n",
    "for train, test in kf:\n",
    "    cv_train_data = X.iloc[train]\n",
    "    cv_test_data = X.iloc[test]\n",
    "    \n",
    "    train_docs = tag_docs(cv_train_data, 'essay_content')\n",
    "    test_docs = tag_docs(cv_test_data, 'essay_content')\n",
    "    model = train_doc2vec_model_new(train_docs)\n",
    "    \n",
    "    y_train, X_train = vec_for_learning(model, train_docs)\n",
    "    y_test, X_test = vec_for_learning(model, test_docs)\n",
    "    \n",
    "    logreg = LogisticRegression()\n",
    "    logreg.fit(X_train, y_train)\n",
    "    y_pred = logreg.predict(X_test)\n",
    "    print accuracy_score(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cv_train_data, cv_test_data = train_test_split(cv_data, test_size=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def tokenize_text(text):\n",
    "    tokens = []\n",
    "    for sent in nltk.sent_tokenize(text):\n",
    "        for word in nltk.word_tokenize(sent):\n",
    "            if len(word) < 2:\n",
    "                continue\n",
    "            tokens.append(word.lower())\n",
    "    return tokens\n",
    "\n",
    "def tag_docs(docs, col):\n",
    "    tagged = docs.apply(lambda r: TaggedDocument(words=r[col], tags=[r.label]), axis=1)\n",
    "    return tagged\n",
    "\n",
    "def train_doc2vec_model_new(tagged_docs, window, size):    \n",
    "    sents = tagged_docs.values\n",
    "    # train doc2vec model to get vector representation of documents\n",
    "    doc2vec_model = Doc2Vec(sents, size=size, window=window, iter=20, dm=1)\n",
    "    return doc2vec_model\n",
    "\n",
    "def vec_for_learning(doc2vec_model, tagged_docs):\n",
    "    sents = tagged_docs.values\n",
    "    targets, regressors = zip(\\\n",
    "            *[(doc.tags[0], doc2vec_model.infer_vector(doc.words, steps=20)) for doc in sents])\n",
    "    return targets, regressors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_docs = tag_docs(cv_train_data, 'essay_content')\n",
    "test_docs = tag_docs(cv_test_data, 'essay_content')\n",
    "model = train_doc2vec_model_new(train_docs, 5, 10)\n",
    "y_train, X_train = vec_for_learning(model, train_docs)\n",
    "y_test, X_test = vec_for_learning(model, test_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.93617021276595747"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logreg = LogisticRegression()\n",
    "logreg.fit(X_train, y_train)\n",
    "y_pred = logreg.predict(X_test)\n",
    "accuracy_score(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Unique words per essay"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Unique words**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_1['unique_words'] = df_1['essay_content'].apply(lambda x: len(set([word.lower().strip(string.punctuation) \\\n",
    "                                                              for word in x.split(' ')])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Stemming**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "stemmer = SnowballStemmer('english')\n",
    "#df_1['unique_stemmer'] = df_1['essay_content'].apply(lambda x: len(set([stemmer.stem(word) for word in [word.lower().strip(string.punctuation) \\\n",
    "                                                              #for word in x.split(' ')]])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Lemmatizing**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'having'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lmtzr = WordNetLemmatizer()\n",
    "lmtzr.lemmatize('having')\n",
    "#stemmer.stem('having')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "lmtzr = WordNetLemmatizer()\n",
    "df_1['unique_lemma'] = df_1['essay_content'].apply(lambda x: len(set([lmtzr.lemmatize(word) for word in [word.lower().strip(string.punctuation) \\\n",
    "                                                              for word in x.split(' ')]])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>doc_id</th>\n",
       "      <th>author_code</th>\n",
       "      <th>essay_content</th>\n",
       "      <th>label</th>\n",
       "      <th>target</th>\n",
       "      <th>unique_words</th>\n",
       "      <th>unique_lemma</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>W_CHN_PTJ0_021_A2_0.txt</td>\n",
       "      <td>﻿I agree that it is important for college students to have apart-time job. Nowadays, a large number of college students are having a part-time job. Some of them hold that part-time jobs can help them to adapt to the society well and give them many experiences. Take a friend of mine for example, when Lily was a college student, she went to supermarket as a promoter or went to be a family teacher every weekend. Then owing to her experiences, she quickly got a good job after graduating from university. in the other students' opinions, they think that they can buy goods they want their parents cannot afford. I know a student that he goes to a restaurant as a waiter at his part-time to earn enough money what a computer needs. Thus, it is important for college students to have a part-time job. However, our parents always don't agree us to get part-time jobs. They are afraid that our study and safety. In my opinion, having a good part-time job is good for students. We should pay attention to the advantages of part-time jobs and make the most of them. Meanwhile, we should learn to get knowledge from the part-time jobs and make them a helpful tool for our development.</td>\n",
       "      <td>CHN</td>\n",
       "      <td>1</td>\n",
       "      <td>115</td>\n",
       "      <td>110</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   doc_id              author_code  \\\n",
       "0  1       W_CHN_PTJ0_021_A2_0.txt   \n",
       "\n",
       "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               essay_content  \\\n",
       "0  ﻿I agree that it is important for college students to have apart-time job. Nowadays, a large number of college students are having a part-time job. Some of them hold that part-time jobs can help them to adapt to the society well and give them many experiences. Take a friend of mine for example, when Lily was a college student, she went to supermarket as a promoter or went to be a family teacher every weekend. Then owing to her experiences, she quickly got a good job after graduating from university. in the other students' opinions, they think that they can buy goods they want their parents cannot afford. I know a student that he goes to a restaurant as a waiter at his part-time to earn enough money what a computer needs. Thus, it is important for college students to have a part-time job. However, our parents always don't agree us to get part-time jobs. They are afraid that our study and safety. In my opinion, having a good part-time job is good for students. We should pay attention to the advantages of part-time jobs and make the most of them. Meanwhile, we should learn to get knowledge from the part-time jobs and make them a helpful tool for our development.   \n",
       "\n",
       "  label  target  unique_words  unique_lemma  \n",
       "0  CHN   1       115           110           "
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_1.head(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test SpaCy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import spacy\n",
    "spc_nlp = spacy.load('en')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "word = spc_nlp(u'I have a saw.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[u'-PRON-', u'have', u'a', u'saw', u'.']"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[token.lemma_ for token in word]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "doc = spc_nlp(u\"I agree that it is important for college students to have apart-time job. Nowadays, a large number of college students are having a part-time job. Some of them hold that part-time jobs can help them to adapt to the society well and give them many experiences. Take a friend of mine for example, when Lily was a college student, she went to supermarket as a promoter or went to be a family teacher every weekend. Then owing to her experiences, she quickly got a good job after graduating from university. in the other students' opinions, they think that they can buy goods they want their parents cannot afford. I know a student that he goes to a restaurant as a waiter at his part-time to earn enough money what a computer needs. Thus, it is important for college students to have a part-time job. However, our parents always don't agree us to get part-time jobs. They are afraid that our study and safety. In my opinion, having a good part-time job is good for students. We should pay attention to the advantages of part-time jobs and make the most of them. Meanwhile, we should learn to get knowledge from the part-time jobs and make them a helpful tool for our development.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "I agree that it is important for college students to have apart-time job. Nowadays, a large number of college students are having a part-time job. Some of them hold that part-time jobs can help them to adapt to the society well and give them many experiences. Take a friend of mine for example, when Lily was a college student, she went to supermarket as a promoter or went to be a family teacher every weekend. Then owing to her experiences, she quickly got a good job after graduating from university. in the other students' opinions, they think that they can buy goods they want their parents cannot afford. I know a student that he goes to a restaurant as a waiter at his part-time to earn enough money what a computer needs. Thus, it is important for college students to have a part-time job. However, our parents always don't agree us to get part-time jobs. They are afraid that our study and safety. In my opinion, having a good part-time job is good for students. We should pay attention to the advantages of part-time jobs and make the most of them. Meanwhile, we should learn to get knowledge from the part-time jobs and make them a helpful tool for our development."
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "e_spacy = set([token.lemma_ for token in doc if token.is_punct==False])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "e_nltk = set([lmtzr.lemmatize(word) for word in [word.lower().strip(string.punctuation) for word in doc.text.split(' ')]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "93"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(e_spacy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "109"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(e_nltk)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compare spaCy and NLTK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dif =  e_spacy^e_nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#dif"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#e_spacy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**lemmatizing**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_1['spacy_lemma'] = df_1['essay_content'].apply(lambda x: len(set([token.lemma_ for token in spc_nlp(x.decode('utf-8')) if token.is_punct==False])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**spaCy doesn not have stemming because they believe that stemming doesn't provide good info**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>doc_id</th>\n",
       "      <th>author_code</th>\n",
       "      <th>essay_content</th>\n",
       "      <th>label</th>\n",
       "      <th>target</th>\n",
       "      <th>unique_words</th>\n",
       "      <th>unique_lemma</th>\n",
       "      <th>spacy_lemma</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>W_CHN_PTJ0_021_A2_0.txt</td>\n",
       "      <td>﻿I agree that it is important for college students to have apart-time job. Nowadays, a large number of college students are having a part-time job. Some of them hold that part-time jobs can help them to adapt to the society well and give them many experiences. Take a friend of mine for example, when Lily was a college student, she went to supermarket as a promoter or went to be a family teacher every weekend. Then owing to her experiences, she quickly got a good job after graduating from university. in the other students' opinions, they think that they can buy goods they want their parents cannot afford. I know a student that he goes to a restaurant as a waiter at his part-time to earn enough money what a computer needs. Thus, it is important for college students to have a part-time job. However, our parents always don't agree us to get part-time jobs. They are afraid that our study and safety. In my opinion, having a good part-time job is good for students. We should pay attention to the advantages of part-time jobs and make the most of them. Meanwhile, we should learn to get knowledge from the part-time jobs and make them a helpful tool for our development.</td>\n",
       "      <td>CHN</td>\n",
       "      <td>1</td>\n",
       "      <td>115</td>\n",
       "      <td>110</td>\n",
       "      <td>94</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   doc_id              author_code  \\\n",
       "0  1       W_CHN_PTJ0_021_A2_0.txt   \n",
       "\n",
       "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               essay_content  \\\n",
       "0  ﻿I agree that it is important for college students to have apart-time job. Nowadays, a large number of college students are having a part-time job. Some of them hold that part-time jobs can help them to adapt to the society well and give them many experiences. Take a friend of mine for example, when Lily was a college student, she went to supermarket as a promoter or went to be a family teacher every weekend. Then owing to her experiences, she quickly got a good job after graduating from university. in the other students' opinions, they think that they can buy goods they want their parents cannot afford. I know a student that he goes to a restaurant as a waiter at his part-time to earn enough money what a computer needs. Thus, it is important for college students to have a part-time job. However, our parents always don't agree us to get part-time jobs. They are afraid that our study and safety. In my opinion, having a good part-time job is good for students. We should pay attention to the advantages of part-time jobs and make the most of them. Meanwhile, we should learn to get knowledge from the part-time jobs and make them a helpful tool for our development.   \n",
       "\n",
       "  label  target  unique_words  unique_lemma  spacy_lemma  \n",
       "0  CHN   1       115           110           94           "
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_1.head(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Average Sentence Length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "doc = spc_nlp(u\"I agree that it is important for college students to have apart-time job. Nowadays, a large number of college students are having a part-time job. Some of them hold that part-time jobs can help them to adapt to the society well and give them many experiences. Take a friend of mine for example, when Lily was a college student, she went to supermarket as a promoter or went to be a family teacher every weekend. Then owing to her experiences, she quickly got a good job after graduating from university. in the other students' opinions, they think that they can buy goods they want their parents cannot afford. I know a student that he goes to a restaurant as a waiter at his part-time to earn enough money what a computer needs. Thus, it is important for college students to have a part-time job. However, our parents always don't agree us to get part-time jobs. They are afraid that our study and safety. In my opinion, having a good part-time job is good for students. We should pay attention to the advantages of part-time jobs and make the most of them. Meanwhile, we should learn to get knowledge from the part-time jobs and make them a helpful tool for our development.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sentences = doc.sents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ss = len([s for s in sentences])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "13"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "avg_slen = np.mean(ss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "13.0"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "avg_slen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_1['avg_stc_length'] = df_1['essay_content'].apply(lambda x: np.mean([len(s.string.strip().split(' ')) for s in spc_nlp(x.decode('utf-8')).sents]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>doc_id</th>\n",
       "      <th>author_code</th>\n",
       "      <th>essay_content</th>\n",
       "      <th>label</th>\n",
       "      <th>target</th>\n",
       "      <th>unique_words</th>\n",
       "      <th>unique_lemma</th>\n",
       "      <th>spacy_lemma</th>\n",
       "      <th>avg_stc_length</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>W_CHN_PTJ0_021_A2_0.txt</td>\n",
       "      <td>﻿I agree that it is important for college students to have apart-time job. Nowadays, a large number of college students are having a part-time job. Some of them hold that part-time jobs can help them to adapt to the society well and give them many experiences. Take a friend of mine for example, when Lily was a college student, she went to supermarket as a promoter or went to be a family teacher every weekend. Then owing to her experiences, she quickly got a good job after graduating from university. in the other students' opinions, they think that they can buy goods they want their parents cannot afford. I know a student that he goes to a restaurant as a waiter at his part-time to earn enough money what a computer needs. Thus, it is important for college students to have a part-time job. However, our parents always don't agree us to get part-time jobs. They are afraid that our study and safety. In my opinion, having a good part-time job is good for students. We should pay attention to the advantages of part-time jobs and make the most of them. Meanwhile, we should learn to get knowledge from the part-time jobs and make them a helpful tool for our development.</td>\n",
       "      <td>CHN</td>\n",
       "      <td>1</td>\n",
       "      <td>115</td>\n",
       "      <td>110</td>\n",
       "      <td>94</td>\n",
       "      <td>16.230769</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   doc_id              author_code  \\\n",
       "0  1       W_CHN_PTJ0_021_A2_0.txt   \n",
       "\n",
       "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               essay_content  \\\n",
       "0  ﻿I agree that it is important for college students to have apart-time job. Nowadays, a large number of college students are having a part-time job. Some of them hold that part-time jobs can help them to adapt to the society well and give them many experiences. Take a friend of mine for example, when Lily was a college student, she went to supermarket as a promoter or went to be a family teacher every weekend. Then owing to her experiences, she quickly got a good job after graduating from university. in the other students' opinions, they think that they can buy goods they want their parents cannot afford. I know a student that he goes to a restaurant as a waiter at his part-time to earn enough money what a computer needs. Thus, it is important for college students to have a part-time job. However, our parents always don't agree us to get part-time jobs. They are afraid that our study and safety. In my opinion, having a good part-time job is good for students. We should pay attention to the advantages of part-time jobs and make the most of them. Meanwhile, we should learn to get knowledge from the part-time jobs and make them a helpful tool for our development.   \n",
       "\n",
       "  label  target  unique_words  unique_lemma  spacy_lemma  avg_stc_length  \n",
       "0  CHN   1       115           110           94           16.230769       "
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_1.head(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Synonyms count in each Doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from nltk.corpus import wordnet as wn\n",
    "def syn(word, lch_threshold=2.26):\n",
    "    for net1 in wn.synsets(word):\n",
    "        for net2 in wn.all_synsets():\n",
    "            try:\n",
    "                lch = net1.lch_similarity(net2)\n",
    "            except:\n",
    "                continue\n",
    "            # The value to compare the LCH to was found empirically.\n",
    "            # (The value is very application dependent. Experiment!)\n",
    "            if lch >= lch_threshold:\n",
    "                yield (net1, net2, lch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Total sentence count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_1['toatl_sents'] = df_1['essay_content'].apply(lambda x: len([s for s in spc_nlp(x.decode('utf-8')).sents]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "\"['unique_stemmer'] not in index\"",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-39-52f83b0d7c13>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdf_1\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'label'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'target'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'unique_words'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'unique_stemmer'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'unique_lemma'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'avg_stc_length'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'toatl_sents'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhead\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/Library/Python/2.7/site-packages/pandas/core/frame.pyc\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   1989\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mSeries\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndarray\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mIndex\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1990\u001b[0m             \u001b[0;31m# either boolean or fancy integer index\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1991\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_getitem_array\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1992\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mDataFrame\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1993\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_getitem_frame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Python/2.7/site-packages/pandas/core/frame.pyc\u001b[0m in \u001b[0;36m_getitem_array\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   2033\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtake\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindexer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconvert\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2034\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2035\u001b[0;31m             \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mix\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_convert_to_indexer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2036\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtake\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindexer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconvert\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2037\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Python/2.7/site-packages/pandas/core/indexing.pyc\u001b[0m in \u001b[0;36m_convert_to_indexer\u001b[0;34m(self, obj, axis, is_setter)\u001b[0m\n\u001b[1;32m   1212\u001b[0m                 \u001b[0mmask\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcheck\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1213\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mmask\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0many\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1214\u001b[0;31m                     \u001b[0;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'%s not in index'\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mobjarr\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mmask\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1215\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1216\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0m_values_from_object\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindexer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: \"['unique_stemmer'] not in index\""
     ]
    }
   ],
   "source": [
    "#df_1[['label', 'target', 'unique_words', 'unique_stemmer', 'unique_lemma', 'avg_stc_length', 'toatl_sents']].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index([u'doc_id', u'author_code', u'essay_content', u'label', u'target',\n",
       "       u'unique_words', u'unique_lemma', u'spacy_lemma', u'avg_stc_length',\n",
       "       u'toatl_sents'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_1.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "\"['unique_stemmer'] not in index\"",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-41-e46bbaea8398>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf_1\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'unique_words'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'unique_stemmer'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'unique_lemma'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'avg_stc_length'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'toatl_sents'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf_1\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'target'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_test_split\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.7\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Python/2.7/site-packages/pandas/core/frame.pyc\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   1989\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mSeries\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndarray\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mIndex\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1990\u001b[0m             \u001b[0;31m# either boolean or fancy integer index\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1991\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_getitem_array\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1992\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mDataFrame\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1993\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_getitem_frame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Python/2.7/site-packages/pandas/core/frame.pyc\u001b[0m in \u001b[0;36m_getitem_array\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   2033\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtake\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindexer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconvert\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2034\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2035\u001b[0;31m             \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mix\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_convert_to_indexer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2036\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtake\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindexer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconvert\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2037\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Python/2.7/site-packages/pandas/core/indexing.pyc\u001b[0m in \u001b[0;36m_convert_to_indexer\u001b[0;34m(self, obj, axis, is_setter)\u001b[0m\n\u001b[1;32m   1212\u001b[0m                 \u001b[0mmask\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcheck\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1213\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mmask\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0many\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1214\u001b[0;31m                     \u001b[0;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'%s not in index'\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mobjarr\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mmask\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1215\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1216\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0m_values_from_object\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindexer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: \"['unique_stemmer'] not in index\""
     ]
    }
   ],
   "source": [
    "X = df_1[['unique_words', 'unique_stemmer', 'unique_lemma', 'avg_stc_length', 'toatl_sents']]\n",
    "y = df_1['target']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, train_size=0.7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "clf = RandomForestClassifier(n_estimators=50)\n",
    "#clf = LogisticRegression()\n",
    "#clf.fit(X_train_dtm[120:140], y_train[120:140])\n",
    "#y_pred = clf.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#accuracy_score(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cross_val_score(clf, X_train, y_train, scoring='accuracy', cv=11)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate POS for Doc2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import spacy\n",
    "spc_nlp = spacy.load('en')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_1['DT_pos'] = df_1['essay_content'].apply(lambda x: [' '.join([token.pos_ for token in spc_nlp(s.text)]) for s in spc_nlp(x.decode('utf-8')).sents])\n",
    "df_1['DT_archs'] = df_1['essay_content'].apply(lambda x: [' '.join([token.dep_ for token in spc_nlp(s.text)]) for s in spc_nlp(x.decode('utf-8')).sents])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#df_1.head(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Doc2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "labeled_train_pos=[]\n",
    "#print train_pos[0:2]\n",
    "train_pos = [['hello','world','alice'],['bob','cat','dog'],['king','queen','job']]\n",
    "for i,list_of_words in enumerate(train_pos):\n",
    "    so=LabeledSentence(words=list_of_words,tags=['train_pos_'])\n",
    "    labeled_train_pos.append(so)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "labeled_train_pos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#cores = multiprocessing.cpu_count()\n",
    "#model = Doc2Vec(min_count=1, window=2, size=100, negative=5, workers=cores)\n",
    "model = Doc2Vec(min_count=1, window=2, size=100, negative=5)\n",
    "sentences = labeled_train_pos\n",
    "model.build_vocab(sentences)\n",
    "\n",
    "# Train the model\n",
    "# This may take a bit to run \n",
    "for i in range(5):\n",
    "    print \"Training iteration %d\" % (i)\n",
    "    #random.shuffle(sentences)\n",
    "    model.train(sentences, total_examples=model.corpus_count, epochs=1)\n",
    "\n",
    "# Use the docvecs function to extract the feature vectors for the training and test data\n",
    "# train_pos_vec=model.docvecs[train_pos_\n",
    "train_pos_vec=[]\n",
    "for i,line in enumerate(train_pos):\n",
    "    train_pos_vec.append(model.docvecs['train_pos_'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "len(train_pos_vec[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "len(train_pos_vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_1['doc2vec_lm_token'] = df_1['essay_content'].apply(lambda x: [token.lemma_ for token in spc_nlp(x.decode('utf-8')) if token.is_punct==False])\n",
    "#df_1['doc2vec_pos_token'] = df_1['DT_pos_archs'].apply(lambda x: [word.lower() for word in x])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#df_1['doc2vec_lm_token']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def label_sentences(df, col):\n",
    "    labeled_sentences = []\n",
    "    for index, datapoint in df.iterrows():\n",
    "        tokenized_words = datapoint[col]\n",
    "        labeled_sentences.append(LabeledSentence(words=tokenized_words, tags=['ESSAY_{}'.format(datapoint['doc_id'])]))\n",
    "    return labeled_sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train_doc2vec_model(labeled_sentences):\n",
    "    model = Doc2Vec(min_count=1, window=2, size=5, negative=20)\n",
    "    model.build_vocab(labeled_sentences)\n",
    "    for epoch in range(10):\n",
    "        model.train(labeled_sentences, total_examples=model.corpus_count, epochs=1)\n",
    "        #model.alpha -= 0.002 \n",
    "        #model.min_alpha = model.alpha\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sen = label_sentences(df_1, 'doc2vec_pos_token')\n",
    "model = train_doc2vec_model(sen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#sen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def vectorize_comments(df,d2v_model, add_col):\n",
    "    y = []\n",
    "    doc_vec = []\n",
    "    for i in range(1,df.shape[0]+1):\n",
    "        label = 'ESSAY_{}'.format(i)\n",
    "        doc_vec.append(d2v_model.docvecs[label])\n",
    "    df[add_col] = doc_vec\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_d2v = vectorize_comments(df_1,model, 'vectorized_essay')\n",
    "#print (df_d2v.head(2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X = df_d2v['vectorized_essay'].T.tolist()\n",
    "y = df_d2v['target']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, train_size=0.7)\n",
    "clf = LogisticRegression()\n",
    "#clf.fit(X_train_dtm[120:140], y_train[120:140])\n",
    "clf.fit(X_train, y_train)\n",
    "y_pred = clf.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "accuracy_score(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sentence Parsing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split essays into smaller chunks (5-6 sentences per chunk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "root = 'data/split1'\n",
    "spc_nlp = spacy.load('en')\n",
    "pwk.split_essay('data/pilot', 'data/split1', spc_nlp)\n",
    "df_orig_split = pwk.load_data(root, 2, 'CHN', 'ENS')\n",
    "df_2 = df_orig_split.drop('path', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sum(df_2['label']=='CHN')*1./df_2.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**spaCy, but spaCy does not support constituency tree yet**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "text = u\"A hearing is scheduled on the issue today.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "doc = spc_nlp(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ss = []\n",
    "for token in doc:\n",
    "    ss.append(token.pos_+' '+token.dep_)#, [child for child in token.children]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "' '.join(ss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**From dependency tree, we can abstract all the archs for each sentence to be a feature**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for token in doc:\n",
    "    print token.dep_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for token in doc:\n",
    "    print token.pos_, token.dep_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#[[token.dep_ for token in spc_nlp(s.text)] for s in doc.sents]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[u'advmod',\n",
       " u'nsubj',\n",
       " u'aux',\n",
       " u'ROOT',\n",
       " u'nsubj',\n",
       " u'ccomp',\n",
       " u'mark',\n",
       " u'pcomp',\n",
       " u'nsubj',\n",
       " u'aux',\n",
       " u'advmod',\n",
       " u'advcl',\n",
       " u'det',\n",
       " u'dobj',\n",
       " u'dobj',\n",
       " u'nsubj',\n",
       " u'aux',\n",
       " u'relcl']"
      ]
     },
     "execution_count": 186,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[token.dep_ for token in spc_nlp(u'Also it will make working ineffective because of we dont do the job which we should do')]\n",
    "                                 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[u'it/nsubjpass can/aux be/auxpass concluded/ROOT that/mark taking/csubj a/det part/compound -/punct time/compound job/dobj is/ccomp rewarding/acomp and/cc eye/compound -/punct opening/conj for/prep those/pobj who/nsubj have/relcl good/amod self/compound -/punct regulation/dobj but/cc it/nsubj may/aux have/conj some/det side/compound effects/dobj if/mark the/det student/nsubj does/aux nt/advmod have/advcl a/det good/amod plan/dobj']"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[' '.join(token.text+'/'+token.dep_ for token in spc_nlp(u'it can be concluded that taking a part-time job is rewarding and eye-opening for those who have good self-regulation but it may have some side effects if the student doesnt have a good plan'))]\n",
    "#[[c for c in token.children] for token in spc_nlp(u'he put the cat on the rug') if token.text=='rug']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### average max child count per essay"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_1['DT_pos'] = df_1['essay_content'].apply(lambda x: [' '.join([token.pos_ for token in spc_nlp(s.text)]) for s in spc_nlp(x.decode('utf-8')).sents])\n",
    "df_1['DT_archs'] = df_1['essay_content'].apply(lambda x: [' '.join([token.dep_ for token in spc_nlp(s.text)]) for s in spc_nlp(x.decode('utf-8')).sents])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_1['DT_avg_dp_cnt'] = df_1['essay_content'].apply(lambda x: [max([len([c for c in token.children]) \\\n",
    "                                                                  for token in spc_nlp(s.text)]) \\\n",
    "                                                                      for s in spc_nlp(x.decode('utf-8')).sents])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#df_1['DT_avg_dp_cnt']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Relative positions of ROOT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[u'nsubj ROOT mark nsubj ccomp acomp mark compound nsubj aux advcl amod punct compound dobj punct',\n",
       " u'advmod punct det amod nsubj prep compound pobj aux ROOT det compound punct compound dobj punct',\n",
       " u'nsubj prep pobj ROOT mark compound punct compound nsubj aux ccomp nsubj aux ccomp prep det pobj advmod cc conj dative amod dobj punct',\n",
       " u'advcl det dobj prep pobj prep pobj punct advmod nsubj advcl det compound attr punct nsubj ROOT prep pobj prep det pobj cc conj aux xcomp det compound attr det npadvmod punct',\n",
       " u'advmod prep prep poss pobj punct nsubj advmod ROOT det amod dobj prep pcomp prep pobj punct',\n",
       " u'prep det amod poss case pobj punct nsubj ROOT mark nsubj aux ccomp dobj nsubj relcl poss nsubj aux neg ccomp punct',\n",
       " u'nsubj ROOT det dobj dobj nsubj relcl prep det pobj prep det pobj prep poss compound punct pobj aux advcl amod dobj dobj det nsubj relcl punct',\n",
       " u'advmod punct nsubj ROOT acomp mark compound nsubj aux advcl det compound punct compound dobj punct',\n",
       " u'advmod punct poss nsubj advmod aux neg ROOT nsubj aux ccomp compound punct compound dobj punct',\n",
       " u'nsubj ROOT acomp punct poss intj cc conj punct',\n",
       " u'prep poss pobj punct csubj det amod compound punct compound dobj ROOT acomp prep pobj punct',\n",
       " u'nsubj aux ROOT dobj prep det pobj prep compound punct compound pobj cc conj det dobj prep pobj punct',\n",
       " u'advmod punct nsubj aux ROOT aux xcomp dobj prep det compound punct compound pobj cc conj nsubj det amod ccomp prep poss pobj punct']"
      ]
     },
     "execution_count": 205,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_1.DT_archs[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### compare the sentence length and the index of ROOT shows that ROOT.index doesn't necessarily reflect the sentence length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{122: [13]},\n",
       " {107: [2]},\n",
       " {29: [2]},\n",
       " {45: [1]},\n",
       " {108: [3]},\n",
       " {65: [6]},\n",
       " {64: [8]},\n",
       " {118: [5]},\n",
       " {48: [2]},\n",
       " {138: [3]},\n",
       " {55: [2]},\n",
       " {167: [4]},\n",
       " {82: [11]},\n",
       " {109: [4]}]"
      ]
     },
     "execution_count": 206,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[{len(s):[i for i,e in enumerate(s.split(' ')) if e=='ROOT']} for s in df_1.DT_archs[1]]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[13, 2, 2, 1, 3, 6, 8, 5, 2, 3, 2, 4, 11, 4]"
      ]
     },
     "execution_count": 207,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[[i for i,e in enumerate(s.split(' ')) if e=='ROOT'][0] for s in df_1.DT_archs[1]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_1['DT_ROOT_idx'] = df_1['DT_archs'].apply(lambda x: [[i for i,e in enumerate(s.split(' ')) if e=='ROOT'][0] for s in x])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0      [1, 9, 3, 16, 8, 8, 1, 3, 7, 1, 11, 2, 4]                           \n",
       "1      [13, 2, 2, 1, 3, 6, 8, 5, 2, 3, 2, 4, 11, 4]                        \n",
       "2      [2, 7, 13, 8, 2, 7, 6, 4, 4, 12, 6, 7, 5, 1, 2, 7, 1, 2, 5]         \n",
       "3      [4, 1, 2, 6, 1, 1, 3, 13, 5, 4, 1, 4, 1, 8, 13, 3, 3, 5]            \n",
       "4      [0, 2, 8, 7, 2, 5, 12, 5, 13, 2, 3, 1, 2, 5, 6, 5, 3]               \n",
       "5      [1, 1, 2, 2, 3, 2, 5, 15, 5, 1, 5, 1, 1, 14, 2]                     \n",
       "6      [8, 1, 2, 5, 1, 2, 1, 3, 7, 10, 6, 8, 4, 5, 2, 4]                   \n",
       "7      [1, 8, 1, 1, 4, 6, 2, 4, 8, 13, 2, 4, 1, 3, 11, 3, 2, 5]            \n",
       "8      [0, 1, 7, 18, 4, 8, 14, 3, 17, 1, 5, 2, 4, 5, 4, 1, 4]              \n",
       "9      [22, 13, 2, 1, 5, 34, 4, 19, 15, 7, 13, 8, 8, 2]                    \n",
       "10     [2, 9, 6, 8, 8, 5, 7, 5, 5, 7, 9, 3, 1]                             \n",
       "11     [0, 20, 4, 16, 5, 15, 4, 4, 8, 5, 13, 1, 4, 6, 4, 7, 2, 10]         \n",
       "12     [11, 6, 1, 3, 1, 4, 6, 3, 1, 11, 1, 0, 3, 2, 7]                     \n",
       "13     [0, 3, 1, 2, 3, 5, 5, 5, 13, 2, 14, 2, 1, 10, 8, 0, 3, 4, 1]        \n",
       "14     [4, 5, 22, 3, 12, 15, 5, 12]                                        \n",
       "15     [2, 6, 3, 10, 4, 5, 6, 10, 10, 1, 1, 7, 3, 2, 7]                    \n",
       "16     [1, 12, 2, 8, 2, 2, 2, 6, 10, 2, 13, 11, 8, 12, 5, 2, 1]            \n",
       "17     [11, 15, 0, 12, 5, 2, 2, 0, 4, 11, 2, 0, 3, 12, 2, 2, 4]            \n",
       "18     [5, 7, 2, 8, 2, 3, 7, 4, 2, 2, 12, 5, 3, 20, 2]                     \n",
       "19     [4, 3, 2, 4, 0, 15, 1, 21, 4, 1, 5]                                 \n",
       "20     [8, 2, 8, 4, 3, 9, 3, 20, 4, 15, 1, 6, 2, 3]                        \n",
       "21     [1, 1, 1, 9, 10, 9, 1, 2, 10, 2, 6, 1, 7, 4, 11, 3]                 \n",
       "22     [2, 2, 1, 3, 3, 11, 1, 5, 6, 5, 3, 15, 11, 13]                      \n",
       "23     [0, 7, 1, 3, 3, 1, 5, 2, 0, 10, 5, 11, 2, 3, 8, 5]                  \n",
       "24     [1, 5, 8, 6, 8, 4, 9, 4, 3, 1, 9, 7, 9, 5, 7, 4, 2]                 \n",
       "25     [0, 1, 2, 2, 2, 5, 2, 2, 7, 2, 1, 16, 6, 2, 3, 2, 2, 7]             \n",
       "26     [4, 1, 5, 6, 5, 6, 1, 2, 6, 1, 3, 3, 10, 15, 6]                     \n",
       "27     [1, 3, 7, 2, 6, 5, 3, 25, 4, 2, 5, 4, 3, 3, 1]                      \n",
       "28     [0, 5, 3, 3, 3, 4, 7, 4, 2, 10, 2, 4, 8]                            \n",
       "29     [8, 0, 1, 8, 14, 10, 7, 1, 7, 0, 0, 25, 4, 3, 10, 14]               \n",
       "                               ...                                         \n",
       "179    [3, 15, 15, 6, 19, 7, 5, 14, 7]                                     \n",
       "180    [1, 1, 5, 1, 7, 1, 5, 6, 15, 6]                                     \n",
       "181    [12, 11, 9, 3, 11, 4, 5, 10]                                        \n",
       "182    [12, 3, 7, 28, 2, 1, 17, 1]                                         \n",
       "183    [1, 2, 2, 3, 9, 8, 2]                                               \n",
       "184    [1, 4, 1, 7, 1, 3]                                                  \n",
       "185    [1, 1, 1, 8, 3, 2, 2, 32]                                           \n",
       "186    [3, 1, 1, 1, 1, 4, 3, 1, 5, 2, 14, 2, 2]                            \n",
       "187    [1, 4, 2, 2, 11, 25, 1, 2, 4, 12, 3, 2]                             \n",
       "188    [1, 1, 2, 5, 0, 13, 5, 2]                                           \n",
       "189    [1, 1, 3, 2, 1, 20, 14]                                             \n",
       "190    [1, 21, 14, 6, 16, 0, 1, 1, 2]                                      \n",
       "191    [6, 10, 1, 5, 13, 2, 1]                                             \n",
       "192    [0, 2, 3, 1, 9, 9, 4, 7, 3, 8]                                      \n",
       "193    [1, 4, 2, 3, 2, 4, 1, 6, 2]                                         \n",
       "194    [1, 1, 1, 11, 7, 4, 2, 1, 5, 3]                                     \n",
       "195    [1, 6, 19, 20, 8, 3, 10, 11]                                        \n",
       "196    [1, 8, 15, 6, 1, 23, 5, 1]                                          \n",
       "197    [1, 3, 18, 3, 2, 5, 4, 3]                                           \n",
       "198    [1, 5, 1, 4, 6, 10, 13, 13, 1]                                      \n",
       "199    [1, 2, 3, 2, 1, 1, 3, 2]                                            \n",
       "200    [7, 4, 7, 3, 3, 1]                                                  \n",
       "201    [4, 5, 7, 6, 2, 12, 8]                                              \n",
       "202    [5, 7, 14, 6, 4, 15, 9, 1]                                          \n",
       "203    [2, 1, 3, 4, 1, 7, 6, 8, 7, 1, 1, 7, 3, 8, 7, 6, 3, 6]              \n",
       "204    [9, 3, 7, 3, 4, 8, 8, 15, 14, 1, 1, 1, 1, 5, 2, 5, 9, 2, 4, 1, 4, 2]\n",
       "205    [1, 1, 13, 12, 1, 13, 1, 3, 3, 13, 9]                               \n",
       "206    [2, 3, 7, 31, 1, 43, 5, 1, 3, 35, 24]                               \n",
       "207    [4, 2, 1, 3, 0, 3, 3, 5, 3, 2, 5]                                   \n",
       "208    [1, 4, 12, 3, 13, 8, 0, 14, 10, 13, 4, 2, 6, 17, 15, 3, 7]          \n",
       "Name: DT_ROOT_idx, dtype: object"
      ]
     },
     "execution_count": 209,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_1.DT_ROOT_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXQAAAD8CAYAAABn919SAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nRvw/v9y1eRPiZ5EAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1a22fd7f50>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "plt.hist(np.array(df_1.DT_ROOT_idx.apply(lambda x: np.array(x).mean())))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Count of xxpass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[u'nsubj ROOT mark nsubj ccomp acomp mark compound nsubj aux advcl amod punct compound dobj punct',\n",
       " u'advmod punct det amod nsubj prep compound pobj aux ROOT det compound punct compound dobj punct',\n",
       " u'nsubj prep pobj ROOT mark compound punct compound nsubj aux ccomp nsubj aux ccomp prep det pobj advmod cc conj dative amod dobj punct',\n",
       " u'advcl det dobj prep pobj prep pobj punct advmod nsubj advcl det compound attr punct nsubj ROOT prep pobj prep det pobj cc conj aux xcomp det compound attr det npadvmod punct',\n",
       " u'advmod prep prep poss pobj punct nsubj advmod ROOT det amod dobj prep pcomp prep pobj punct',\n",
       " u'prep det amod poss case pobj punct nsubj ROOT mark nsubj aux ccomp dobj nsubj relcl poss nsubj aux neg ccomp punct',\n",
       " u'nsubj ROOT det dobj dobj nsubj relcl prep det pobj prep det pobj prep poss compound punct pobj aux advcl amod dobj dobj det nsubj relcl punct',\n",
       " u'advmod punct nsubj ROOT acomp mark compound nsubj aux advcl det compound punct compound dobj punct',\n",
       " u'advmod punct poss nsubj advmod aux neg ROOT nsubj aux ccomp compound punct compound dobj punct',\n",
       " u'nsubj ROOT acomp punct poss intj cc conj punct',\n",
       " u'prep poss pobj punct csubj det amod compound punct compound dobj ROOT acomp prep pobj punct',\n",
       " u'nsubj aux ROOT dobj prep det pobj prep compound punct compound pobj cc conj det dobj prep pobj punct',\n",
       " u'advmod punct nsubj aux ROOT aux xcomp dobj prep det compound punct compound pobj cc conj nsubj det amod ccomp prep poss pobj punct']"
      ]
     },
     "execution_count": 175,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_1.DT_archs[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_1['DT_pass_cnt'] = df_1['DT_archs'].apply(lambda x: [len([dep for dep in s.split(' ') if dep[-4:]=='pass']) for s in x])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Count of 'mark'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    [2, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0]                  \n",
       "1    [0, 2, 0, 0, 1, 0, 1, 0, 0, 2, 0, 0, 1, 0]               \n",
       "2    [0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 2, 0, 1]\n",
       "Name: DT_archs, dtype: object"
      ]
     },
     "execution_count": 212,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#df_1['DT_pass_cnt'] = df_1['DT_archs'].apply(lambda x: [len([dep for dep in s.split(' ') if dep[-4:]=='pass']) for s in x])\n",
    "df_1['DT_archs'][:3].apply(lambda x: [len([dep for dep in s.split(' ') if dep=='mark']) for s in x])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_1['DT_mark_cnt'] = df_1['DT_archs'].apply(lambda x: [len([dep for dep in s.split(' ') if dep=='mark']) for s in x])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Repetition of adjective POS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[u'NOUN VERB ADP PRON VERB ADJ ADP NOUN NOUN PART VERB ADV PUNCT NOUN NOUN PUNCT',\n",
       " u'ADV PUNCT DET ADJ NOUN ADP NOUN NOUN VERB VERB DET ADJ PUNCT NOUN NOUN PUNCT',\n",
       " u'DET ADP PRON VERB DET NOUN PUNCT NOUN NOUN VERB VERB PRON PART VERB ADP DET NOUN ADV CCONJ VERB PRON ADJ NOUN PUNCT',\n",
       " u'VERB DET NOUN ADP NOUN ADP NOUN PUNCT ADV PROPN VERB DET NOUN NOUN PUNCT PRON VERB ADP NOUN ADP DET NOUN CCONJ VERB PART VERB DET NOUN NOUN DET NOUN PUNCT',\n",
       " u'ADV VERB ADP ADJ NOUN PUNCT PRON ADV VERB DET ADJ NOUN ADP VERB ADP NOUN PUNCT',\n",
       " u'ADP DET ADJ NOUN PART NOUN PUNCT PRON VERB ADP PRON VERB VERB NOUN PRON VERB ADJ NOUN VERB ADV VERB PUNCT',\n",
       " u'PRON VERB DET NOUN ADJ PRON VERB ADP DET NOUN ADP DET NOUN ADP ADJ ADJ PUNCT NOUN PART VERB ADJ NOUN NOUN DET NOUN VERB PUNCT',\n",
       " u'ADV PUNCT PRON VERB ADJ ADP NOUN NOUN PART VERB DET ADJ PUNCT NOUN NOUN PUNCT',\n",
       " u'ADV PUNCT ADJ NOUN ADV VERB ADV VERB PRON PART VERB ADJ PUNCT NOUN NOUN PUNCT',\n",
       " u'PRON VERB ADJ ADP ADJ NOUN CCONJ NOUN PUNCT',\n",
       " u'ADP ADJ NOUN PUNCT VERB DET ADJ NOUN PUNCT NOUN NOUN VERB ADJ ADP NOUN PUNCT',\n",
       " u'PRON VERB VERB NOUN ADP DET NOUN ADP NOUN PUNCT NOUN NOUN CCONJ VERB DET ADJ ADP PRON PUNCT',\n",
       " u'ADV PUNCT PRON VERB VERB PART VERB NOUN ADP DET ADJ PUNCT NOUN NOUN CCONJ VERB PRON DET ADJ NOUN ADP ADJ NOUN PUNCT']"
      ]
     },
     "execution_count": 214,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_1.DT_pos[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_1['POS_adjv_body'] = df_1['essay_content'].apply(lambda x: [token.text.lower() for token in spc_nlp(x.decode('utf-8')) if (token.dep_=='amod')|(token.dep_=='advmod')])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0     [(good, 2)]                                \n",
       "1     [(most, 4), (important, 3)]                \n",
       "2     [(great, 3), (hard, 2)]                    \n",
       "3     [(all, 2)]                                 \n",
       "4     [(many, 4), (very, 3)]                     \n",
       "5     []                                         \n",
       "6     [(so, 3)]                                  \n",
       "7     [(so, 3), (much, 2)]                       \n",
       "8     [(so, 4), (when, 2)]                       \n",
       "9     [(very, 4)]                                \n",
       "10    [(so, 3), (more, 3)]                       \n",
       "11    [(valuable, 2)]                            \n",
       "12    [(how, 4), (more, 4)]                      \n",
       "13    [(more, 2), (strong, 2)]                   \n",
       "14    [(more, 5)]                                \n",
       "15    [(more, 5), (only, 2), (most, 2)]          \n",
       "16    [(also, 4), (own, 3)]                      \n",
       "17    [(good, 2)]                                \n",
       "18    [(large, 2), (more, 2)]                    \n",
       "19    [(just, 2), (strongly, 1)]                 \n",
       "20    [(more, 4), (just, 3)]                     \n",
       "21    [(all, 2)]                                 \n",
       "22    [(more, 3), (different, 2), (also, 2)]     \n",
       "23    [(own, 2), (greater, 2)]                   \n",
       "24    [(more, 4), (so, 3), (only, 2)]            \n",
       "25    [(many, 3), (much, 3)]                     \n",
       "26    [(all, 2)]                                 \n",
       "27    [(different, 2), (most, 2)]                \n",
       "28    [(so, 4), (even, 1)]                       \n",
       "29    [(all, 2), (other, 2)]                     \n",
       "30    [(many, 4), (how, 2)]                      \n",
       "31    [(good, 3), (all, 2), (really, 2)]         \n",
       "32    [(so, 4), (many, 2)]                       \n",
       "33    [(more, 3)]                                \n",
       "34    [(only, 2)]                                \n",
       "35    [(all, 2), (so, 2), (secondly, 1)]         \n",
       "36    [(also, 4)]                                \n",
       "37    [(more, 4)]                                \n",
       "38    [(great, 2), (only, 2)]                    \n",
       "39    [(poor, 2)]                                \n",
       "40    [(theoretical, 6), (social, 4)]            \n",
       "41    [(social, 3), (extra, 2)]                  \n",
       "42    [(when, 2), (free, 2)]                     \n",
       "43    [(more, 6), (working, 3)]                  \n",
       "44    [(more, 4), (real, 2)]                     \n",
       "45    [(when, 3), (more, 3)]                     \n",
       "46    [(more, 4), (different, 3)]                \n",
       "47    [(very, 2), (when, 2)]                     \n",
       "48    [(very, 3), (so, 3)]                       \n",
       "49    [(practical, 2), (theoretical, 2), (so, 2)]\n",
       "Name: POS_adjv_body, dtype: object"
      ]
     },
     "execution_count": 244,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_1['POS_adjv_body'][:50].apply(lambda x: Counter(x).most_common(len(Counter(x).keys())/9)) # Get top 1/9 most common terms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Count of PUNCT+FANBOYS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Boundary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# archs\n",
    "#df_2['DT_archs'] = df_2['essay_content'].apply(lambda x: [' '.join([token.dep_ for token in spc_nlp(s.text)]) for s in spc_nlp(x.decode('utf-8')).sents])\n",
    "df_1['DT_archs'] = df_1['essay_content'].apply(lambda x: [' '.join([token.dep_ for token in spc_nlp(s.text)]) for s in spc_nlp(x.decode('utf-8')).sents])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# pos\n",
    "#df_2['DT_pos'] = df_2['essay_content'].apply(lambda x: [' '.join([token.pos_ for token in spc_nlp(s.text)]) for s in spc_nlp(x.decode('utf-8')).sents])\n",
    "df_1['DT_pos'] = df_1['essay_content'].apply(lambda x: [' '.join([token.pos_ for token in spc_nlp(s.text)]) for s in spc_nlp(x.decode('utf-8')).sents])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# pos+archs\n",
    "#df_2['DT_pos_archs'] = df_2['essay_content'].apply(lambda x: [' '.join([token.pos_+' '+token.dep_ for token in spc_nlp(s.text)]) for s in spc_nlp(x.decode('utf-8')).sents])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\xef\\xbb\\xbfI agree that it is important for college students to have apart-time job. Nowadays, a large number of college students are having a part-time job. Some of them hold that part-time jobs can help them to adapt to the society well and give them many experiences. Take a friend of mine for example, when Lily was a college student, she went to supermarket as a promoter or went to be a family teacher every weekend. Then owing to her experiences, she quickly got a good job after graduating from university. in the other students' opinions, they think that they can buy goods they want their parents cannot afford. I know a student that he goes to a restaurant as a waiter at his part-time to earn enough money what a computer needs. Thus, it is important for college students to have a part-time job. However, our parents always don't agree us to get part-time jobs. They are afraid that our study and safety. In my opinion, having a good part-time job is good for students. We should pay attention to the advantages of part-time jobs and make the most of them. Meanwhile, we should learn to get knowledge from the part-time jobs and make them a helpful tool for our development.\""
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_1.essay_content[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for i in xrange(df_2.shape[0]):\n",
    "    if len(set(df_2.iloc[i,6]))!= len(df_2.iloc[i,6]):\n",
    "        print len(set(df_2.iloc[i,6]))\n",
    "        print len(df_2.iloc[i,6])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pwk.ngram('A hearing is scheduled on the issue today', 4, ' ')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create dependency tree archs ngram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_1['DT_pos_join'] = df_1['DT_pos'].apply(lambda x: ' '.join(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### When using POS ngram, as n increases, the accuracy increases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X = df_1['DT_pos_join']\n",
    "#X = df_2['essay_content']\n",
    "y = df_1['target']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, train_size=0.6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X = df_1['essay_content']\n",
    "#X = df_2['essay_content']\n",
    "y = df_1['target']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, train_size=0.6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pipeline_mnb = Pipeline([\n",
    "    ('vect', TfidfVectorizer(lowercase=True, ngram_range=(2,2))),\n",
    "    ('clf', MultinomialNB())\n",
    "])\n",
    "scores = cross_val_score(pipeline_mnb, X_train, y_train, cv=5)\n",
    "print scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train_dtm.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X = df_2['DT_pos_join']\n",
    "#X = df_2['essay_content']\n",
    "y = df_2['target']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, train_size=0.6)\n",
    "vectorizer = TfidfVectorizer(ngram_range=(3,3))\n",
    "X_train_dtm = vectorizer.fit_transform(X_train)\n",
    "X_test_dtm = vectorizer.transform(X_test)\n",
    "#clf = LinearSVC()\n",
    "clf = LogisticRegression()\n",
    "clf.fit(X_train_dtm, y_train)\n",
    "y_pred = clf.predict(X_test_dtm)\n",
    "print accuracy_score(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "n = 4\n",
    "df_2['DT_archs_ngram'] = df_2['DT_archs'].apply(lambda x: sum([pwk.ngram(s, n, ' ') for s in x],[]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_2.head(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get the most common arch patterns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Counter(df_2.iloc[3,8]).most_common(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create POS ngram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "n = 4\n",
    "df_1['DT_pos_ngram'] = df_1['DT_pos'].apply(lambda x: sum([pwk.ngram(s, n, ' ') for s in x],[]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_2.iloc[0,6]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Counter(df_1.iloc[0,14]).most_common(7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('this', 'is', 'a')\n",
      "('is', 'a', 'foo')\n",
      "('a', 'foo', 'bar')\n",
      "('foo', 'bar', 'sentences')\n",
      "('bar', 'sentences', 'and')\n",
      "('sentences', 'and', 'i')\n",
      "('and', 'i', 'want')\n",
      "('i', 'want', 'to')\n",
      "('want', 'to', 'ngramize')\n",
      "('to', 'ngramize', 'it')\n"
     ]
    }
   ],
   "source": [
    "from nltk import ngrams\n",
    "sentence = 'this is a foo bar sentences and i want to ngramize it'\n",
    "n = 3\n",
    "sixgrams = ngrams(sentence.split(), n)\n",
    "for grams in sixgrams:\n",
    "  print grams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "u'NOUN VERB ADP PRON VERB ADJ ADP NOUN NOUN PART VERB ADV PUNCT NOUN NOUN PUNCT'"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_1['DT_pos'][:1].values[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#df_1['DT_pos'][:1].apply(lambda x: pwk.loop_body(x, 3))\n",
    "n = 3\n",
    "df_1['DT_insent_pos_ngram'] = df_1['DT_pos'].apply(lambda x: pwk.loop_body(x, n))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#df_1['DT_insent_pos_ngram']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "**Stanford CoreNLP**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pycorenlp import StanfordCoreNLP\n",
    "nlp = StanfordCoreNLP('http://localhost:9000')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "text = \"It is important for college students to have apart-time job.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "output = nlp.annotate(text, properties={\n",
    "        'annotators': 'tokenize,ssplit,pos,depparse,parse',\\\n",
    "        'outputFormat': 'json'\n",
    "    })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print output['sentences'][0]['parse']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simple Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X = df_1['essay_content']\n",
    "y = df_1['target']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, train_size=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(y_train==1)*1./len(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nfrom nltk.stem.snowball import PorterStemmer\\n\\nstemmer = PorterStemmer()\\nanalyzer = TfidfVectorizer(ngram_range=(2,2)).build_analyzer()\\n\\ndef stemmed_words(doc):\\n    return (stemmer.stem(w) for w in analyzer(doc))\\n'"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "from nltk.stem.snowball import PorterStemmer\n",
    "\n",
    "stemmer = PorterStemmer()\n",
    "analyzer = TfidfVectorizer(ngram_range=(2,2)).build_analyzer()\n",
    "\n",
    "def stemmed_words(doc):\n",
    "    return (stemmer.stem(w) for w in analyzer(doc))\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#vectorizer = CountVectorizer(lowercase=True, analyzer=stemmed_words)\n",
    "vectorizer = CountVectorizer(lowercase=True, max_features=10)\n",
    "#vectorizer = TfidfVectorizer(lowercase=True, use_idf=False, stop_words='english', ngram_range=(1,3))\n",
    "#vectorizer = CountVectorizer(lowercase=True, stop_words='english', ngram_range=(2,2))\n",
    "X_train_dtm = vectorizer.fit_transform(X_train)\n",
    "X_test_dtm = vectorizer.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10, 10)"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_dtm.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "terms = np.array(vectorizer.get_feature_names())#[-5:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "clf = LogisticRegression()\n",
    "#clf.fit(X_train_dtm[120:140], y_train[120:140])\n",
    "clf.fit(X_train_dtm, y_train)\n",
    "y_pred = clf.predict(X_test_dtm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.80000000000000004"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_score(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.svm import LinearSVC\n",
    "clf = SGDClassifier()\n",
    "clf.fit(X_train_dtm, y_train) \n",
    "y_pred = clf.predict(X_test_dtm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "accuracy_score(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for i in xrange(10):\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, train_size=0.6)\n",
    "    vectorizer = TfidfVectorizer(lowercase=True)\n",
    "    X_train_dtm = vectorizer.fit_transform(X_train)\n",
    "    X_test_dtm = vectorizer.transform(X_test)\n",
    "    clf = LogisticRegression()\n",
    "    clf.fit(X_train_dtm, y_train)\n",
    "    y_pred = clf.predict(X_test_dtm)\n",
    "    print accuracy_score(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#vectorizer = CountVectorizer(lowercase=True, analyzer=stemmed_words)\n",
    "vectorizer = CountVectorizer(lowercase=True)\n",
    "#vectorizer = TfidfVectorizer(lowercase=True, use_idf=False, stop_words='english', ngram_range=(2,2))\n",
    "#vectorizer = CountVectorizer(lowercase=True, stop_words='english', ngram_range=(2,2))\n",
    "X_train_dtm = vectorizer.fit_transform(X_train)\n",
    "X_test_dtm = vectorizer.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "clf = MultinomialNB()\n",
    "#clf.fit(X_train_dtm[:100], y_train.iloc[:100])\n",
    "clf.fit(X_train_dtm, y_train)\n",
    "y_pred2 = clf.predict(X_test_dtm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "accuracy_score(y_test, y_pred2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y_pred2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "a = [1,2,3,4,5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "a[1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for i in range(120,140):\n",
    "    clf = MultinomialNB()\n",
    "    clf.fit(X_train_dtm[i:140], y_train[i:140])\n",
    "    y_p = clf.predict(X_test_dtm)\n",
    "    print '{}:'.format(i), accuracy_score(y_test, y_p) \n",
    "    print y_train.iloc[i]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### It turns out that if we control the topic, the vocabulary plays a significant role in classifying Chinese over native speakers, especially for Logistic Regression. Naive Bayes in this case doesn't perform well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "idx = clf.coef_[0].argsort()[::-1][:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "terms[idx]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Explore high tfidf score terms for each class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "vectorizer2 = TfidfVectorizer(lowercase=True, use_idf=False, stop_words='english')\n",
    "#vectorizer = CountVectorizer(lowercase=True)\n",
    "df_CHN = df_1[df_1['label']=='CHN']['essay_content']\n",
    "test_dtm = vectorizer2.fit_transform(df_CHN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_dtm.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_term = np.array(vectorizer2.get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "total = np.array(np.sum(test_dtm, axis=0))[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_term[total.argsort()[::-1][:10]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "vectorizer3 = TfidfVectorizer(lowercase=True, use_idf=False, stop_words='english')\n",
    "#vectorizer = CountVectorizer(lowercase=True)\n",
    "df_ENS = df_1[df_1['label']=='ENS']['essay_content']\n",
    "test_dtm_en = vectorizer3.fit_transform(df_ENS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_dtm_en.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_term_en = np.array(vectorizer3.get_feature_names())\n",
    "total_en = np.array(np.sum(test_dtm_en, axis=0))[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_term_en[total_en.argsort()[::-1][:10]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
